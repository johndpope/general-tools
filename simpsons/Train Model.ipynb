{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import keras.applications\n",
    "import keras.preprocessing.image\n",
    "from keras.layers import *\n",
    "from keras.regularizers import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import *\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import PIL\n",
    "\n",
    "import scipy\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import math\n",
    "\n",
    "import sys\n",
    "import logging \n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger(\"py.warnings\").setLevel(logging.ERROR)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import general\n",
    "from general.plotting import gridplot\n",
    "general.dl.make_keras_picklable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import simpsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.12.1', '1.2.0', '1.12.1')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__, keras.__version__, np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/cpu:0', '/gpu:0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "[x.name for x in local_device_protos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    training_img_shape=(202,360),     # 1/4 of HD image\n",
    "    test_img_shape=(202,360),         # ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "with gzip.GzipFile(\"/data/simpsons/dataset.pickle.zip\", \"r\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images by char:  [ 91.  79.  64.  37.]\n",
      "train_dev images by char:  [ 30.  26.  22.  12.]\n",
      "dev  images by char:  [  74.  101.   66.   65.]\n",
      "test images by char:  [  73.  102.   64.   64.]\n",
      "train\n",
      "  - X: (474, 200, 200, 3)\n",
      "  - y: (474, 4)\n",
      "train_dev\n",
      "  - X: (158, 200, 200, 3)\n",
      "  - y: (158, 4)\n",
      "dev\n",
      "  - X: (351, 202, 360, 3)\n",
      "  - y: (351, 4)\n",
      "test\n",
      "  - X: (351, 202, 360, 3)\n",
      "  - y: (351, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"train images by char: \", data.train.y.sum(axis=0))\n",
    "print(\"train_dev images by char: \", data.train_dev.y.sum(axis=0))\n",
    "print(\"dev  images by char: \", data.dev.y.sum(axis=0))\n",
    "print(\"test images by char: \", data.test.y.sum(axis=0))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmenting training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%store -r background_images\n",
    "if 'background_images' not in vars(): \n",
    "    frame_generator = simpsons.preprocessing.SimpsonsFrameGenerator(\n",
    "        background_videoclips_path=\"/data/simpsons/preprocessing/training/backgrounds\",\n",
    "        background_required_num=100, \n",
    "        background_output_shape=config['training_img_shape'],\n",
    "    )\n",
    "    background_images = frame_generator.background_images\n",
    "    %store background_images    \n",
    "    \n",
    "    import pickle\n",
    "    import gzip\n",
    "    import time\n",
    "\n",
    "    with gzip.GzipFile(\"/data/simpsons/background_images.pickle.zip\", 'wb') as f:\n",
    "        pickle.dump(background_images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(opt='adam', fc_detectors=[], train_all=False, batchnorm=True, dropout=0.5, base_output_layer='block5_pool', \n",
    "                 num_detectors=16, num_outputs=4, reg_penalty=0.0):\n",
    "    if type(base_output_layer) is int:\n",
    "        base_output_layer = \"block{}_pool\".format(base_output_layer)\n",
    "\n",
    "    unknown_img_size_inp = Input(shape=(None,None,3), name='input')\n",
    "    vgg = keras.applications.vgg16.VGG16(include_top=False)\n",
    "    base_model = Model(input=vgg.input, output=vgg.get_layer(base_output_layer).output)\n",
    "    \n",
    "    base_model_output = x = base_model(unknown_img_size_inp)\n",
    "    \n",
    "    if batchnorm:\n",
    "        x = BatchNormalization(name='bn')(x)\n",
    "    x = Dropout(dropout, name=\"dropout\")(x)\n",
    "\n",
    "    for ind, fc_det in enumerate(fc_detectors):\n",
    "        x = Convolution2D(fc_det, 1, 1, name=\"fc_detector_{}\".format(ind), W_regularizer=keras.regularizers.l2(reg_penalty))(x)\n",
    "    \n",
    "    x = Convolution2D(num_detectors,1,1, name='detectors_spatial', W_regularizer=keras.regularizers.l2(reg_penalty))(x)\n",
    "    x = GlobalMaxPooling2D(name='detectors')(x)\n",
    "    \n",
    "    x = Dense(num_outputs, name='output', activation='sigmoid')(x)\n",
    "       \n",
    "    model = Model(input=unknown_img_size_inp, output=x) \n",
    "    \n",
    "    # Compile model    \n",
    "    if not train_all:\n",
    "        for l in base_model.layers:\n",
    "            l.trainable = False\n",
    "    \n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mae', 'accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________________________________________________________________________________\n",
      "Layer (type)                         Output Shape              Param #       Connected to                          \n",
      "===================================================================================================================\n",
      "input (InputLayer)                   (None, None, None, 3)     0                                                   \n",
      "___________________________________________________________________________________________________________________\n",
      "model_1 (Model)                      (None, None, None, 512)   14714688      input[0][0]                           \n",
      "___________________________________________________________________________________________________________________\n",
      "bn (BatchNormalization)              (None, None, None, 512)   2048          model_1[1][0]                         \n",
      "___________________________________________________________________________________________________________________\n",
      "dropout (Dropout)                    (None, None, None, 512)   0             bn[0][0]                              \n",
      "___________________________________________________________________________________________________________________\n",
      "detectors_spatial (Convolution2D)    (None, None, None, 16)    8208          dropout[0][0]                         \n",
      "___________________________________________________________________________________________________________________\n",
      "detectors (GlobalMaxPooling2D)       (None, 16)                0             detectors_spatial[0][0]               \n",
      "___________________________________________________________________________________________________________________\n",
      "output (Dense)                       (None, 4)                 68            detectors[0][0]                       \n",
      "===================================================================================================================\n",
      "Total params: 14,725,012\n",
      "Trainable params: 9,300\n",
      "Non-trainable params: 14,715,712\n",
      "___________________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = create_model()\n",
    "m.summary(line_length=115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to train for a single character only (bart), single model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choose characters\n",
    "chars_chooser = simpsons.preprocessing.CharactersChooser([0,1,2,3])  # bart,homer,lisa,marge\n",
    "train_X, train_y         = chars_chooser.transform(data.train.X, data.train.y)\n",
    "train_dev_X, train_dev_y = chars_chooser.transform(data.train_dev.X, data.train_dev.y)\n",
    "dev_X, dev_y             = chars_chooser.transform(data.dev.X, data.dev.y)\n",
    "\n",
    "\n",
    "# Create ConstantCVSplitter\n",
    "all_train_x, all_train_y, cv_splitter = general.utils.ConstantCVSplitter(\n",
    "    train_X, train_y, train_dev_X, train_dev_y\n",
    ").split()\n",
    "\n",
    "# Custom Simpsons scorer\n",
    "simpsons_scorer = simpsons.scoring.SimpsonsDevsetScorer(dev_X, dev_y)\n",
    "\n",
    "# Preprocess pipeline\n",
    "simpsons_preprocess = Pipeline([\n",
    "        ('vgg_preprocessor', simpsons.preprocessing.VGGPreprocessing()),\n",
    "    ])\n",
    "\n",
    "# simpsons frame generator\n",
    "frame_generator = simpsons.preprocessing.SimpsonsFrameGenerator(\n",
    "    preprocess_pipeline=simpsons_preprocess,\n",
    "    background_images=background_images, \n",
    "    background_videoclips_path=\"/data/simpsons/preprocessing/training/backgrounds\",\n",
    "    background_required_num=100, \n",
    "    background_output_shape=config['training_img_shape'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run model only and visualize learning curve (loss/epoch)\n",
    "build_params = [\n",
    "    {\n",
    "        'num_outputs': [4],\n",
    "        'batchnorm': [False],\n",
    "        'num_detectors': [128],\n",
    "        'fc_detectors': [[]],\n",
    "        'reg_penalty': [ 0.001 ],\n",
    "    }\n",
    "]\n",
    "generator_params = [\n",
    "    {\n",
    "        'batch_size': [32],\n",
    "        'output_shape': [ config['training_img_shape'] ],\n",
    "        'max_num_characters': [4],\n",
    "        'train_shape_range': [[0.3,1.0]],        \n",
    "    }\n",
    "]\n",
    "\n",
    "gch = general.dl.DLGridSearch(create_model, build_params=build_params,\n",
    "                generator_fn=frame_generator.generate, generator_params=generator_params)\n",
    "\n",
    "gch.fit(data.train.X, data.train.y, train_dev_X, train_dev_y, nb_epoch=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general.plotting.KerasHistoryPlotter().plot(gch.results, ylim=(0.,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simpsons.scoring.SimpsonsReporter(\n",
    "        train_dev_X, train_dev_y, dev_X, dev_y, \n",
    "        dev_preprocess=simpsons_preprocess, generator_fn=frame_generator.generate,\n",
    "        train_dev_num_images=500\n",
    ").report(gch.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id = 0\n",
    "\n",
    "model = gch.results[id].history.model\n",
    "params = gch.results[id].params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# with gzip.GzipFile(\"data/gch.pickle.gz\", \"rb\") as f:\n",
    "#     gch = pickle.load(f)\n",
    "    \n",
    "with gzip.GzipFile(\"data/gch.pickle.gz\", \"wb\") as f:\n",
    "    pickle.dump(gch, f)\n",
    "\n",
    "print(\"saving model for \", params)\n",
    "model.save(\"data/model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"/data/simpsons/model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize classifier's thresholds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_X = data.dev.X\n",
    "dev_y = data.dev.y\n",
    "dev_preds = model.predict(simpsons_preprocess.transform(dev_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cls = general.utils.ClassifierThresholdOptimizer(model, None, dev_y, preds=dev_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cls.fit(maximize_fscore=True)\n",
    "preds_fscore = cls.predict(None, preds=dev_preds)\n",
    "\n",
    "cls.fit(maximize_accuracy=True)\n",
    "preds_acc = cls.predict(None, preds=dev_preds)\n",
    "\n",
    "cls.fit(recall=0.75)\n",
    "preds_rec_075 = cls.predict(None, preds=dev_preds)\n",
    "\n",
    "cls.fit(precision=0.75)\n",
    "preds_pre_075 = cls.predict(None, preds=dev_preds)\n",
    "\n",
    "num_imgs = 100\n",
    "general.plotting.gridplot_sidebyside(\n",
    "    [dev_X[:num_imgs], preds_pre_075[:num_imgs]], \n",
    "    [dev_X[:num_imgs], preds_rec_075[:num_imgs]], \n",
    "    [dev_X[:num_imgs], preds_fscore[:num_imgs]], \n",
    "    [dev_X[:num_imgs], preds_acc[:num_imgs]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.65,  0.55,  0.4 ,  0.4 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy was chosen as the best one\n",
    "cls.fit(maximize_accuracy=True)\n",
    "cls.thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store final model and thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_and_thresholds = { \"model\": model, \"preprocess_pipeline\": simpsons_preprocess, \"thresholds\": cls.thresholds }\n",
    "pickle.dump(model_and_thresholds, open(\"/data/simpsons/model_and_thresholds.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Show some generated training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from general.plotting import gridplot\n",
    "frame_generator = simpsons.preprocessing.SimpsonsFrameGenerator(\n",
    "    background_images=background_images)\n",
    "\n",
    "g = frame_generator.generate(data.train.X, data.train.y, max_num_characters=3, batch_size=32,\n",
    "                             output_shape=config['training_img_shape'], train_shape_range=[0.3,1.0])\n",
    "gen_imgs, gen_labels = next(g)\n",
    "gridplot(gen_imgs, titles=gen_labels, num_cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Train Log\n",
    "\n",
    "## Trying to overfit on small samples and scale up\n",
    "* 1 BG, 10 TRAIN, NO DIST, 25 epochs - overfitted perfectly.\n",
    "* 1 BG, 50 TRAIN, NO DIST, 25 epochs - overfitted nicely. some positives get mid scores (0.4-0.6)\n",
    "* 1 BG, ~500 TRAIN, NO DIST, 500 epochs - overfitted. get to loss = ~0.08 with a nice decrease over epochs. Validation stays flat.\n",
    "\n",
    "__Apparently all above were run with a validation of pure data (not generated = no bg image).__  \n",
    "__Apparently all above were run with vgg_means in scale of (0,255) and img in scale of (0,1).__\n",
    "\n",
    "Fixing vgg_means and train_dev_generator:\n",
    "\n",
    "* 1 BG, 50 TRAIN, NO DIST, 25 epochs - training loss drop to 0 pretty quickly. validation at ~0.05.\n",
    "\n",
    "Trying with all training images.\n",
    "\n",
    "* 1 BG, ~500 TRAIN, NO DIST, 25 epochs - train and val drops pretty much together to ~0.01-0.04 (loss). Results on dev are horrible. probably because of the single background image in train.\n",
    "* ALL BG, ~500 TRAIN, NO DIST, 25 epochs - train and val to ~0.05. no overfitting. performs a bit better on dev but still sucks..\n",
    "\n",
    "Adding more backgrounds (~100 now). Cleaning training images. Training images are now 100x200.  \n",
    "Adding distortions back:\n",
    "\n",
    "* ALL BG, ALL TRAIN, WITH DISTS, 40 epochs - no overfitting. train and train_dev together to ~0.05 (loss). validation looks nice. misses some (mostly when bart's hair is not in the image). might be because it was only trained for 40 epochs and didn't see a lot of distortions.\n",
    "\n",
    "## Testing hyperparameters\n",
    "\n",
    "1. what level to hook into? __clearly 5__\n",
    "1. with/without first batchnorm (right after VGG)? __with__\n",
    "1. dropout ratio (with first batchnorm) - __0.1, 0.25 and 0.5 are acceptable__\n",
    "1. with/without second batchnorm (after detectors, before last FC) and dropout ratios - __without second batchnorm, 0 and 0.25 are fine__\n",
    "1. trainall vs. traindelta? __clearly traindelta__\n",
    "\n",
    "\n",
    "## Comparing performances on train_dev and dev\n",
    "\n",
    "All settings are: ALL BG, ALL TRAIN, WITH DISTS, train_all=False, base_output_layer=5, batchnorm=True, dropout=0.25\n",
    "\n",
    "| settings | train_dev performance | dev performance | comments |\n",
    "| --- | --- | --- | --- |\n",
    "| default | loss ~0.05, no overfitting. AUC=0.94, 80%r=70%p | AUC=0.67, 80%r=10%p | maybe overfitting the train_dev. trying to increase dropout. |\n",
    "| try with more zoom (0.9,2.5), 500 epochs, droprout=0.5 | noisy. loss ~0.08. AUC=0.65, 80%r=15%p | AUC=0.62, 80%r=5%p | looked at bart imgs in train_dev and dev and sorted by score. small barts get low score. large barts are identified easily. should try to train on 100x100 and move the patch on the frame |\n",
    "| no shift_range, rotation=10.0, more zoom (1.0, 3.5), 500 epochs, dropout=0.5 | flat after epoch 200, loss ~0.13. AUC=0.82, 80%r=0%p|  AUC=0.65, 80%r=2%p| |\n",
    "\n",
    "Changing training images back to 100x100. trying to get perfect fit on train_dev\n",
    "\n",
    "| settings | train_dev performance | dev performance | comments |\n",
    "| --- | --- | --- | --- |\n",
    "| train 100x100, zoom 1.0-1.5, no other dists, 50 epochs | loss ~0.08, AUC=0.87, 80%r=75%p | AUC=0.66 , 80%r=5%p | |\n",
    "\n",
    "Changing training images - removing scaling to 100x100. every image is centered in black background of 100x100 but unscaled\n",
    "\n",
    "| settings | train_dev performance | dev performance | comments |\n",
    "| --- | --- | --- | --- |\n",
    "| train 100x100, zoom 1.0-1.5, no other dists, 50 epochs | AUC=0.95 , 80%r=70%p | AUC=0.61  , 80%r=0%p |  |\n",
    "\n",
    "Train images are 100x100. Changing dev images to 202x360 (50% of orig).\n",
    "\n",
    "| settings | train_dev performance | dev performance | comments |\n",
    "| --- | --- | --- | --- |\n",
    "| no dists, 100 epochs | AUC=1.00 , 80%r=100%p | AUC=0.69 , 80%r=40%p | By looking at dev, I believe the main problem is the black edge on training images  |\n",
    "| changed train gen to reduce black edge. 50 epochs | AUC=0.74 , 80%r=30%p | AUC=0.82 , 80%r=40%p | |\n",
    "| \\*same\\*.  training 50 epochs more. __Also calculating train_dev stats on many iterations__ | AUC=0.96 , 80%r=90%p | AUC=0.81 , 80%r=45%p | |\n",
    "| \\*same\\*.  training 100 epochs more. | AUC=0.96 , 80%r=90%p | AUC=0.78 , 80%r=50%p | looks much better when examining dev. should try to add more dists and check if it's better to predict on the whole dev image or slide across it |\n",
    "| adding dists: rotate 10.0, zoom: [0.9,1.5]. comparing dropout=0,25, 0.5. 100 epochs | | | | \n",
    "| \\* _dropout=0.25_ | AUC=0.94, 80%r=80%p | full: AUC=0.77, 80%r=50%p. patches: AUC=0.8, 80%r=50%p | |\n",
    "| \\* _dropout=0.5_ | AUC=0.92, 80%r=70%p | full: AUC=0.77, 80%r=50%p. patches: AUC=0.81, 80%r=40%p | | \n",
    "| 100 epochs more (dropout=0.25) | AUC=0.92, 80%r=70%p | full: AUC=0.77, 80%r=35%p. patches: AUC=0.79, 80%r=55%p | | |\n",
    "| 500 epochs more (dropout=0.25) | AUC=0.97, 80%r=80%p | full: AUC=0.75, 80%r=45%p. patches: AUC=0.78, 80%r=30%p | | |\n",
    "\n",
    "## Adding marge\n",
    "\n",
    "| settings | train_dev performance || dev performance || comments |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| | __AUC__ | __% prec for 80% recall__ | __AUC__ | __% prec for 80% recall__ | |\n",
    "| 200 epochs | 0.97, 0.94 | 80%, 75% | 0.73, 0.86 | 35%, 50% | should try with more detectors |\n",
    "| tried with 32,64,128 detetors. lost the data. keep with 32 detectors ||||||\n",
    "| > Focusing on making train closer to dev. Mainly by image manipulation. I believe a better training images generation and a better patch_img_on_background process will get significant results. <br/><br/> changing training image rescaling method to __'lanczos'__ (looks better with the characters edges). Also removed all dists. 100 epochs | 0.98, 0.96 | 85%, 95% | 0.67, 0.84 | 30%, 40% | |\n",
    "| changed black sensitivity in patch_img_on_background (0.3 -> 0.1). 100 epochs | 0.95, 0.9 | 85%, 80%| 0.67, 0.86 | 20%, 40% | | \n",
    "| adding dists back: (100 epochs) | | | | | |\n",
    "| + zoom [0.9, 1.2] | 0.96, 0.98 | 85%, 90% | 0.64, 0.82 | 85%, 40% | * 85%?? reading error? |\n",
    "| + rotation 10.0 | 0.98, 0.98 | 80%, 85% | 0.64, 0.74 | 25%, 20% | should remove rotation. it makes images grainy on the edges. Also - scale 0.8 looks a bit better |\n",
    "| removing rotation. zoom only [0.9, 1.5] | 0.94, 0.92 | 80%, 80% | 0.66, 0.78 | 15%, 35% | scaled dev (0.7) brings results to 0.70, 0.81, 15%, 50% |\n",
    "| training 100 epochs more (200 total) | 0.97, 0.94 | 85%, 65% | 0.69, 0.83 | 20%, 50% | |\n",
    "| no dists, 200 epochs | 0.99, 1.00 | 95%, 95% | 0.67, 0.74 | 20%, 30% | scaled (0.7) brings marge to 0.84 and 50%. In general, scale 0.7 seems optimal for AUC in both characters | \n",
    "\n",
    "Increasing training size to (150,150). Still bart and marge only\n",
    "\n",
    "| settings | train_dev performance || dev performance || comments |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| | __AUC__ | __% prec for 80% recall__ | __AUC__ | __% prec for 80% recall__ | |\n",
    "| no dists, 100 epochs | 0.99, 0.97 | 95%, 90% | 0.69, 0.90 | 30%, 70%| |\n",
    "| with zoom [1.0, 1.5], 100 epochs | 0.98, 0.96 | 90%, 80% | 0.69, 0.87 | 30%, 55% | |\n",
    "| no dists. 200 epochs | 0.97, 0.96 | 90%, 90% | 0.70, 0.85 | 35%, 65% | |\n",
    "| AUC/epoch analysis shows that there is no increasing of AUC after 100 epochs. | | | | | should try on all characters |\n",
    "| Added blur to patch_img_on_background. no dists. 100 epochs | 1.00, 1.00 | 100%, 95% | 0.74, 0.86 | 40%, 60% | |\n",
    "\n",
    "\n",
    "Keeping the blur on training images.  \n",
    "\n",
    "## All characters\n",
    "\n",
    "| settings | train_dev performance || dev performance || comments |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| | __AUC__ | __% prec for 80% recall__ | __AUC__ | __% prec for 80% recall__ | |\n",
    "| no dists, 100 epochs. 16 detectors | ~1.00 | ~90% | 0.65,0.86,0.66,0.38,0.76 | 35%,55%,5%,0%,20%| |\n",
    "| no dists, 100 epochs. 32 detectors | ~1.00 | ~90%-95% | 0.68,0.79,0.64,0.38,0.82 | 30%,20%,5%,0%,50% | |\n",
    "| no dists, 100 epochs. 48 detectors | ~0.98 | ~90%-100% | 0.65,0.82,0.65,0.54,0.86 | 20%,20%,15%,0%,50%| |\n",
    "| no dists, 100 epochs. 64 detectors | ~1.00 | ~90%-100% | 0.65,0.85,0.64,0.46,0.85 | 15%,40%,10%,0%,60%| |\n",
    "\n",
    "Observations: \n",
    "* detectors num - keep it 48 for now. not much of a difference\n",
    "* number of epochs - helps only marge.\n",
    "* severe problem with maggie. \n",
    "* maggie gets better results on zoom > 1.5 (others - not significant impact)\n",
    "* a lot of misses on small characters (weird with the previous bullet)\n",
    "* maybe should measure \"maximal acceptance precision\" as a lot of the images that contain a character, contains it poorly.\n",
    "\n",
    "| settings | train_dev performance || dev performance || comments |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| | __AUC__ | __% prec for 80% recall__ | __AUC__ | __% prec for 80% recall__ | |\n",
    "| adding zoom [1.0, 2.0], 100 epochs (48 detectors) | ~0.95 | ~70%-90% | 0.72,0.74,0.63,0.42,0.78 | 15%,5%,5%,0%,35% | |\n",
    "| no dists. fc_detectors=[64]. 100 epochs | ~1.00 | ~90% | 0.66,0.79,0.66,0.54,0.84 | 30%,30%,10%,0%,50% | |\n",
    "| no dists. fc_detectors=[64,64]. 50 epochs | ~1.00 | ~90%-100% | 0.63,0.8,0.59,0.35,0.85 | 20%,10%,20%,0%,55% | |\n",
    "\n",
    "\n",
    "## Zero-score dev pics analysis\n",
    "\n",
    "| Character | # pics in dev | # should find | % should find | % > 0.5 | # super hard | % super hard | comments | \n",
    "| --- |\n",
    "| Bart | 38 | 25 | 65% | 34% | 11 | 29% | Some super-hards with baseball cap on. In some, Bart is very small (50x70). In some - turning his back |\n",
    "| Homer | 48 | 41 | 85% | 40% | 4 | 8% | Missing some where Homer is small (opening frames) but in some he is very large in the frame |\n",
    "| Lisa | 40 | 20 | 50% | 15% | 8 | 20% | A lot of small Lisa's |\n",
    "| Maggie | 14 | 8 | 57% | 0% | 3 | 21% | All get 0.0. Single one gets 0.08. Maggie is very small in almost all pics (~70x70). |\n",
    "| Marge | 48 | 38 |80% | 33% | 10 | 20% | Turning her back. few too large, few too little |\n",
    "\n",
    "Conclusions: \n",
    "* Should work on \"multi-size\" model.\n",
    "* Check why adding parts to an image (taking larger areas) reduces (sometimes dramatically) the score\n",
    "  * Try to remove BatchNorm. It changes detectors outputs when overall image is changing..\n",
    "\n",
    "| settings | train_dev performance || dev performance || comments |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| | __AUC__ | __% prec for 80% recall__ | __AUC__ | __% prec for 80% recall__ | |\n",
    "| trying without batchnorm. 100 epochs | ~0.98 | ~90%-100%| 0.71,0.84,0.62,0.56,0.83| 20%,15%,20%,10%,40%| |  \n",
    "| trying a \"small model\" vs a \"large model\" | | | | | |\n",
    "| * small model - all training imgs with zoom=2.0 | 0.89,0.87,0.86,0.85,0.96 | 15%-70% | 0.70,0.73,0.59,0.48,0.74 | 10%-40% | |\n",
    "| * large model - all training imgs with zoom=1.0 | 0.99,0.99,0.98,1.00,0.97 | 50%-100% | 0.67,0.83,0.60,0.62,0.87 | 10%-50% | | \n",
    "\n",
    "Changing the train generator:   \n",
    "* Using rescale to create smaller images\n",
    "* Cropping empty borders so character fills the entire image in all sizes\n",
    "\n",
    "Adding a \"combined PR curve\" that measures recall when setting both large and small models on the same precision.\n",
    "\n",
    "| settings | train_dev performance || dev performance || comments |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| | __AUC__ | __% prec for 80% recall__ | __AUC__ | __% prec for 80% recall__ | |\n",
    "| No dists. small=(100,100) | 1.00,0.99,1.00,1.00,1.00 | 95%-100% | 0.60,0.81,0.61,0.59,0.77 | 10%-50% | |\n",
    "| No dists. large=(150,150) | 1.00,1.00,1.00,1.00,0.97 | 90%-100% | 0.61,0.87,0.63,0.63,0.84 | 10%-65% | |\n",
    "| Combined | | | | 15%-65% | |\n",
    "| | | | | | |\n",
    "| *Changing dropout to 0.2*, Adding dists: 5.0 rotation, 0.05 shift, 0.1 zoom. Trying various sizes of training images | | | | | | \n",
    "| tiny (50,50) | 0.95,0.92,0.93,0.97,0.90 | 50%-70% |      0.63,0.48,0.56,0.59,0.61 | 10%-35% | |\n",
    "| v.small (75,75) | 1.00,0.99,0.99,1.00,0.99 | 85%-100% |  0.63,0.80,0.63,0.61,0.60 | 10%-50% | |\n",
    "| small (100,100) | 1.00,1.00,1.00,1.00,1.00 | 100%-100% | 0.59,0.76,0.64,0.59,0.75 | 10%-45% | |\n",
    "| large (150,150) | 1.00,1.00,1.00,1.00,1.00 | 100%-100% | 0.62,0.89,0.66,0.57,0.82 | 10%-75% | |\n",
    "| v.large (200,200) | 1.00,1.00,1.00,1.00,1.00 | 100%-100% | 0.61,0.92,0.64,0.56,0.78 | 10%-75% | |\n",
    "\n",
    "\n",
    "### New test set - 700 frames from 100 episodes.\n",
    "\n",
    "| settings | train_dev performance || dev performance || comments |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| | __AUC__ | __% prec for 80% recall__ | __AUC__ | __% prec for 80% recall__ | |\n",
    "| small (100,100) | 0.98,0.96,0.99,1.00,0.98 | 60%-100% | 0.66,0.65,0.67,0.44,0.75 | 5%-35% | |\n",
    "| large | 1.00,0.99,1.00,1.00,1.00 | 100%-100% | 0.68,0.78,0.72,0.47,0.83 | 5%-45% | |\n",
    "| trying different # dets | \n",
    "| 32 dets | 0.99,0.98,0.99,0.97,0.94 | 60%-100% |  0.71,0.79,0.67,0.43,0.83 | 5%-45% | | \n",
    "| 64 dets | 0.99,0.99,0.99,0.99,0.96 | 65%-100% |  0.72,0.81,0.68,0.41,0.83 | 5%-50% | |\n",
    "| 128 dets | 0.99,0.99,0.99,0.98,0.99 | 70%-100% | 0.71,0.81,0.69,0.41,0.84 | 5%-55% | |\n",
    "| 256 dets | 0.54,1.00,0.99,1.00,0.96 | 20%-100% | 0.45,0.77,0.72,0.42,0.81 | 5%-45% | weird.. got stuck on characters multiple times |\n",
    "| 512 dets | 0.99,1.00,0.99,0.75,0.96 | 15%-100% | 0.72,0.78,0.71,0.43,0.83 | 5%-45% | |\n",
    "| 1024 dets | 0.99,1.00,0.99,0.75,0.59 | 5%-100% | 0.69,0.79,0.71,0.47,0.36 | 5%-45% | |\n",
    "| 2048 dets | 1.00,1.00,0.98,0.78,0.45 | 5%-100% | 0.73,0.81,0.68,0.43,0.33 | 5%-50% | |\n",
    "| 128 dets, different dropouts | \n",
    "| 128 dets, dropout=0.5 | 1.00,1.00,0.99,1.00,1.00 | 100%-100% | 0.72,0.78,0.75,0.43,0.79 | 5%-45% | |\n",
    "| 128 dets, dropout=0.2 | 1.00,1.00,1.00,1.00,1.00 | 100%-100% | 0.75,0.80,0.72,0.40,0.79 | 5%-50% | |\n",
    "| 128 dets, dropout=0.2, different num_epochs | \n",
    "| 10 epochs | 0.55,0.93,0.95,0.94,0.74 | 10%-75% |    0.43,0.79,0.62,0.42,0.54 | 5%-45% | |\n",
    "| 20 epochs | 0.51,0.98,0.98,0.96,0.95 | 20%-100% |   0.43,0.80,0.67,0.37,0.73 | 5%-45% | |\n",
    "| 30 epochs | 0.88,0.98,0.99,0.98,0.95 | 25%-100% |   0.64,0.80,0.66,0.37,0.82 | 5%-50% | |\n",
    "| 40 epochs | 0.99,0.99,0.99,1.00,0.96 | 70%-100% |   0.74,0.78,0.70,0.41,0.81 | 5%-45% | |\n",
    "| 50 epochs | 0.99,0.99,1.00,1.00,0.96 | 85%-100% |   0.74,0.79,0.66,0.38,0.81 | 5%-45% | |\n",
    "| 50 epochs | 1.00,1.00,0.99,0.99,0.98 | 80%-100% |   0.74,0.77,0.69,0.41,0.83 | 5%-45% | |\n",
    "| 60 epochs | 0.99,1.00,0.99,1.00,1.00 | 95%-100% |   0.74,0.79,0.67,0.40,0.81 | 5%-45% | |\n",
    "| 100 epochs | 1.00,1.00,0.99,1.00,0.99 | 100%-100% | 0.73,0.75,0.71,0.47,0.82 | 5%-45% | |\n",
    "| 150 epochs | 1.00,1.00,1.00,1.00,1.00 | 100%-100% | 0.71,0.78,0.73,0.44,0.80 | 5%-45% | |\n",
    "| 200 epochs | 1.00,1.00,1.00,1.00,1.00 | 100%-100% | 0.72,0.78,0.72,0.44,0.80 | 5%-45% | |\n",
    "| 250 epochs | 1.00,1.00,1.00,1.00,1.00 | 100%-100% | 0.71,0.79,0.75,0.41,0.82 | 5%-45% | |\n",
    "| 300 epochs | 1.00,1.00,1.00,1.00,1.00 | 100%-100% | 0.70,0.77,0.75,0.40,0.80 | 5%-45% | |\n",
    "| 350 epochs | 1.00,1.00,1.00,1.00,1.00 | 100%-100% | 0.71,0.79,0.75,0.40,0.81 | 5%-45% | |\n",
    "| 400 epochs | 1.00,1.00,1.00,1.00,1.00 | 100%-100% | 0.71,0.79,0.74,0.40,0.80 | 5%-45% | |\n",
    "| 450 epochs | 1.00,1.00,1.00,1.00,1.00 | 100%-100% | 0.70,0.77,0.74,0.39,0.81 | 5%-40% | |\n",
    "| 500 epochs | 1.00,1.00,1.00,1.00,1.00 | 100%-100% | 0.72,0.78,0.76,0.39,0.83 | 5%-45% | |\n",
    "| 550 epochs | 1.00,1.00,1.00,1.00,1.00 | 100%-100% | 0.74,0.78,0.74,0.38,0.80 | 5%-40% | |\n",
    "| 600 epochs | 1.00,1.00,1.00,1.00,1.00 | 100%-100% | 0.74,0.78,0.75,0.37,0.80 | 5%-45% | |\n",
    "\n",
    "| settings | train_dev performance || dev performance || comments |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| | __AUC__ | __% prec for 80% recall__ | __AUC__ | __% prec for 80% recall__ | |\n",
    "| small (75,75), 60 epochs |   0.98,0.98,0.98,0.98,0.90 | 25%-95% |  0.71,0.65,0.61,0.36,0.64 | 5%-40% | |\n",
    "| large (200,200), 60 epochs | 1.00,1.00,0.98,0.97,0.97 | 55%-100% | 0.73,0.80,0.70,0.44,0.82 | 5%-50% | |\n",
    "| combined | \n",
    "\n",
    "Regenerating training images to be (200,200). completely ignoring images that are less than (200,200) (in both dimentions).  \n",
    "** Also: removing maggie **  \n",
    "** adding stratified splitting of train/test sets**\n",
    "\n",
    "| settings | train_dev performance || dev performance || comments |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| | __AUC__ | __% prec for 80% recall__ | __AUC__ | __% prec for 80% recall__ | |\n",
    "| small (75,75), 200 epochs |   1.00,0.97,1.00,0.99 | 90%-100% |  0.73,0.67,0.63,0.71 | 25%-35% | |\n",
    "| large (200,200), 200 epochs | 1.00,1.00,1.00,1.00 | 100%-100% | 0.76,0.72,0.72,0.81 | 30%-40% | |\n",
    "| combined | | | | 30.0,40.0,25.0,35.0 |\n",
    "| |\n",
    "| Adding zoom 0.5-1.5 to both large and small |\n",
    "| small (75,75), 100 epochs | 0.93,0.92,0.96,0.96 | 55%-85% | 0.71,0.57,0.59,0.68 | 20%-30% |\n",
    "| large (200,200), 100 epochs | 0.99,0.99,0.99,0.99 | 70%-100% | 0.76,0.76,0.69,0.83 | 25%-45% |\n",
    "| combined, 100 epochs | | | | 30.0,40.0,20.0,35.0 |\n",
    "| small (75,75), 250 epochs | 0.98,0.94,0.95,0.99 | 70%-90% | 0.72,0.63,0.64,0.73 | 25%-35% | |\n",
    "| large (200,200), 250 epochs | 1.00,0.99,1.00,1.00 | 95%-100% | 0.74,0.76,0.69,0.79 | 25%-45% | |\n",
    "| combined, 250 epochs | | | | 30.0,45.0,25.0,35.0 |\n",
    "| |\n",
    "| Training image is now at size of background with the character placed randomaly inside. Adding zoom [1.0,2.0], trying a single model | |\n",
    "| 100 epochs | 0.98,0.99,0.99,1.00 | 95%-100% |  0.82,0.75,0.71,0.82 | 25%-45% | |\n",
    "| 200 epochs | 1.00,0.98,1.00,1.00 | 95%-100% |  0.80,0.75,0.72,0.82 | 25%-40% | |\n",
    "| 300 epochs | 1.00,0.98,0.99,1.00 | 95%-100% |  0.81,0.73,0.72,0.82 | 25%-40% | |\n",
    "| 400 epochs | 1.00,1.00,1.00,1.00 | 100%-100% | 0.78,0.75,0.73,0.82 | 25%-40% | |\n",
    "| | \n",
    "| Trying large and small, 100 epochs |\n",
    "| small - output_shape=(100,100) | 0.92,0.89,0.95,0.97 | 50%-80% |   0.72,0.63,0.67,0.77 | 25%-35% |\n",
    "| large - output_shape=None (bg) | 1.00,1.00,1.00,0.93 | 100%-100% | 0.72,0.72,0.71,0.73 | 25%-40% |\n",
    "| combined | | | | 35.0,40.0,25.0,30.0 |\n",
    "| |\n",
    "| Changing zoom mechanism to change the size of the whole training image, thus making it \"moving\" on the background. |\n",
    "| single model - output_shape=None, train_shape_range=[0.3,1.0], 200 epochs | 0.99,0.99,1.00,0.99 | 95%-100% | 0.78,0.75,0.73,0.82 | 25%-40% | |\n",
    "| small - output_shape=(100,100), train_shape_range=[0.6,1.0], 200 epochs | 0.98,0.97,0.96,0.99 | 75%-90% | 0.71,0.64,0.70,0.61 | 25%-35% |  |\n",
    "| large - output_shape=None, train_shape_range=[0.6,1.0], 200 epochs | 1.00,1.00,1.00,1.00 | 100%-100% | 0.73,0.70,0.72,0.79 | 25%-35% | |\n",
    "| small+large | | | | 35.0,35.0,25.0,30.0 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure \"acceptable\" recall rates per character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_images_per_char = 50\n",
    "dev_with_chars = [ \n",
    "    np.random.permutation(np.argwhere(data.dev.y[:,char_id] == 1).flatten())[:num_images_per_char]\n",
    "    for char_id in range(data.dev.y.shape[1])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.82,  0.86,  0.92,  0.86])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_acceptable_misses = np.array([ 9, 7, 4, 7])\n",
    "EXPECTED_RECALL = (1 - num_acceptable_misses / num_images_per_char)\n",
    "EXPECTED_RECALL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gridplot(data.dev.X[dev_with_chars[3]], num_cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
